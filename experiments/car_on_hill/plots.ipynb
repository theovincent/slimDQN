{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "from experiments.car_on_hill.plot_utils import plot_on_grid, plot_value\n",
    "from slimRL.sample_collection.utils import load_valid_transitions\n",
    "from experiments.car_on_hill.optimal import NX, NV\n",
    "\n",
    "BASE_PATH = \"exp_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting for generating the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these value before running the cells below\n",
    "experiments_to_run = [\"test_local/FQI\"]\n",
    "# Set these value to True if you want to generate these plots\n",
    "SAMPLES_DISTRIBUTION = True\n",
    "PERFORMANCE_LOSS = True\n",
    "APPROX_ERROR_ON_RB_DATA = True\n",
    "BELLMAN_ITERATIONS_FOR_POLICY = [1, 5, 10, 30]\n",
    "BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS = [1, 5, 10, 30]\n",
    "OVERESTIMATION_ERROR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check and load all the relevant metrics for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORMANCE_LOSS:\n",
    "    OPT_Q = np.load(\"Q*.npy\")\n",
    "if len(BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS) > 0:\n",
    "    OPT_V = np.load(\"V*.npy\")\n",
    "experiment_seed_folder_map = {\n",
    "    experiment: [\n",
    "        seed_run\n",
    "        for seed_run in os.listdir(os.path.join(BASE_PATH, experiment))\n",
    "        if not os.path.isfile(os.path.join(BASE_PATH, experiment, seed_run))\n",
    "    ]\n",
    "    for experiment in experiments_to_run\n",
    "}\n",
    "assert all(\n",
    "    [\n",
    "        os.path.exists(os.path.join(BASE_PATH, experiment_folder_path))\n",
    "        for experiment_folder_path in experiment_seed_folder_map\n",
    "    ]\n",
    "), \"Found experiments with no corresponing folders\"\n",
    "metrics = {}\n",
    "for experiment in experiment_seed_folder_map:\n",
    "    metrics[experiment] = {}\n",
    "    if SAMPLES_DISTRIBUTION:\n",
    "        metrics[experiment].setdefault(\n",
    "            \"samples_distribution\",\n",
    "            np.load(os.path.join(BASE_PATH, experiment, \"samples_stats.npy\")),\n",
    "        )\n",
    "        metrics[experiment].setdefault(\n",
    "            \"rewards_distribution\",\n",
    "            np.load(os.path.join(BASE_PATH, experiment, \"rewards_stats.npy\")),\n",
    "        )\n",
    "    if (\n",
    "        PERFORMANCE_LOSS\n",
    "        or len(BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS) > 0\n",
    "        or OVERESTIMATION_ERROR\n",
    "    ):\n",
    "        metrics[experiment].setdefault(\n",
    "            \"samples_distribution\",\n",
    "            np.load(os.path.join(BASE_PATH, experiment, \"samples_stats.npy\")),\n",
    "        )\n",
    "        metrics[experiment].setdefault(\n",
    "            \"scaling\",\n",
    "            metrics[experiment][\"samples_distribution\"]\n",
    "            / metrics[experiment][\"samples_distribution\"].sum(),\n",
    "        )\n",
    "    if PERFORMANCE_LOSS:\n",
    "        metrics[experiment].setdefault(\n",
    "            \"q_pi\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"q_pi.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )\n",
    "    if APPROX_ERROR_ON_RB_DATA:\n",
    "        metrics[experiment].setdefault(\n",
    "            \"Tq_rb\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"Tq_rb.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )\n",
    "        metrics[experiment].setdefault(\n",
    "            \"q_rb\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"q_rb.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )\n",
    "        metrics[experiment].setdefault(\n",
    "            \"rb_store\",\n",
    "            load_valid_transitions(\n",
    "                os.path.join(BASE_PATH, experiment, \"replay_buffer.json\")\n",
    "            ),\n",
    "        )\n",
    "    if len(BELLMAN_ITERATIONS_FOR_POLICY) > 0:\n",
    "        metrics[experiment].setdefault(\n",
    "            f\"q_grid\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"q_grid.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )\n",
    "    if len(BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS) > 0 or OVERESTIMATION_ERROR:\n",
    "        metrics[experiment].setdefault(\n",
    "            \"q_pi\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"q_pi.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )\n",
    "        metrics[experiment].setdefault(\n",
    "            \"q_grid\",\n",
    "            {\n",
    "                seed_run: np.load(\n",
    "                    os.path.join(BASE_PATH, experiment, seed_run, \"q_grid.npy\")\n",
    "                )\n",
    "                for seed_run in experiment_seed_folder_map[experiment]\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot samples distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLES_DISTRIBUTION:\n",
    "    TITLES = {\n",
    "        \"samples_distribution\": \"Samples distribution\",\n",
    "        \"rewards_distribution\": \"Rewards distribution\",\n",
    "    }\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        plot_on_grid(\n",
    "            values={\n",
    "                TITLES[p]: metrics[experiment][p]\n",
    "                for p in [\"samples_distribution\", \"rewards_distribution\"]\n",
    "            },\n",
    "            shared_cmap=False,\n",
    "            zeros_to_nan=True,\n",
    "            title=\"Replay buffer statistics\",\n",
    "            fontsize=15,\n",
    "            title_fontsize=20,\n",
    "        ).savefig(\n",
    "            os.path.join(BASE_PATH, experiment, \"samples_distribution.pdf\"),\n",
    "            bbox_inches=\"tight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $\\Vert Q^{*} - Q^{\\pi_i}\\Vert_{1, \\mu_{D}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORMANCE_LOSS:\n",
    "    q_perf_loss = {}\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        n_seeds = len(metrics[experiment][\"q_pi\"])\n",
    "        n_bellman_iterations = list(metrics[experiment][\"q_pi\"].values())[0].shape[0]\n",
    "        q_perf_loss[experiment] = np.zeros((n_seeds, n_bellman_iterations))\n",
    "        for idx_seed, seed_run in enumerate(experiment_seed_folder_map[experiment]):\n",
    "            q_perf_loss[experiment][idx_seed] = np.sqrt(\n",
    "                np.sum(\n",
    "                    np.abs(OPT_Q.reshape(-1, 2) - metrics[experiment][\"q_pi\"][seed_run])\n",
    "                    * metrics[experiment][\"scaling\"].reshape(-1)[:, np.newaxis],\n",
    "                    axis=(1, 2),\n",
    "                )\n",
    "            )\n",
    "    plot_value(\n",
    "        \"Bellman iteration $i$\",\n",
    "        \"$\\Vert Q^{*} - Q^{\\pi_i}\\Vert_{1, \\mu_{D}}$\",\n",
    "        np.arange(0, n_bellman_iterations).tolist(),\n",
    "        q_perf_loss,\n",
    "        ticksize=10,\n",
    "        title=\"Performance loss\",\n",
    "        fontsize=20,\n",
    "        linewidth=3,\n",
    "        xlim=(0, n_bellman_iterations),\n",
    "        xticks=[0]\n",
    "        + [\n",
    "            idx * 10 ** (int(np.log10(n_bellman_iterations)))\n",
    "            for idx in range(\n",
    "                1,\n",
    "                (\n",
    "                    n_bellman_iterations\n",
    "                    // (10 ** (int(np.log10(n_bellman_iterations))))\n",
    "                    + 1\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ).savefig(\n",
    "        os.path.join(BASE_PATH, experiments_to_run[0], \"performance_loss_q.pdf\"),\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $\\Vert \\Gamma Q_{i-1} - Q_i\\Vert^2_{2, \\mu_{D}}$ on replay buffer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPROX_ERROR_ON_RB_DATA:\n",
    "    approx_error_on_rb = {}\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        n_seeds = len(metrics[experiment][\"q_rb\"])\n",
    "        n_bellman_iterations = list(metrics[experiment][\"q_rb\"].values())[0].shape[0]\n",
    "        approx_error_on_rb[experiment] = np.zeros((n_seeds, n_bellman_iterations - 1))\n",
    "        for idx_seed, seed_run in enumerate(experiment_seed_folder_map[experiment]):\n",
    "            approx_error_on_rb[experiment][idx_seed] = np.sum(\n",
    "                np.square(\n",
    "                    metrics[experiment][\"Tq_rb\"][seed_run][:-1]\n",
    "                    - metrics[experiment][\"q_rb\"][seed_run][1:][\n",
    "                        np.arange(n_bellman_iterations - 1)[:, None],\n",
    "                        np.arange(len(metrics[experiment][\"rb_store\"][\"actions\"])),\n",
    "                        metrics[experiment][\"rb_store\"][\"actions\"],\n",
    "                    ]\n",
    "                )\n",
    "                / metrics[experiment][\"samples_distribution\"].sum(),\n",
    "                axis=1,\n",
    "            )\n",
    "    plot_value(\n",
    "        \"Bellman iteration $i$\",\n",
    "        \"$\\Vert \\Gamma Q_{i-1} - Q_i\\Vert^2_{2, \\mu_{D}}$\",\n",
    "        np.arange(1, n_bellman_iterations).tolist(),\n",
    "        approx_error_on_rb,\n",
    "        ticksize=10,\n",
    "        title=\"Approximation error\",\n",
    "        fontsize=20,\n",
    "        linewidth=3,\n",
    "        xlim=(0, n_bellman_iterations - 1),\n",
    "        xticks=[1]\n",
    "        + [\n",
    "            idx * 10 ** (int(np.log10(n_bellman_iterations)))\n",
    "            for idx in range(\n",
    "                1,\n",
    "                (n_bellman_iterations // (10 ** (int(np.log10(n_bellman_iterations)))))\n",
    "                + 1,\n",
    "            )\n",
    "        ],\n",
    "    ).savefig(\n",
    "        os.path.join(BASE_PATH, experiments_to_run[0], \"approx_error_rb_data.pdf\"),\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot policy $\\pi_i(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(BELLMAN_ITERATIONS_FOR_POLICY) > 0:\n",
    "    policies = {}\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        for iteration in BELLMAN_ITERATIONS_FOR_POLICY:\n",
    "            n_seeds = len(metrics[experiment][\"q_grid\"])\n",
    "            policy = np.zeros((n_seeds, NX * NV))\n",
    "            for idx_seed, seed_run in enumerate(experiment_seed_folder_map[experiment]):\n",
    "                policy[idx_seed] = (\n",
    "                    metrics[experiment][\"q_grid\"][seed_run][iteration][:, 1]\n",
    "                    > metrics[experiment][\"q_grid\"][seed_run][iteration][:, 0]\n",
    "                ).astype(float)\n",
    "            policy = np.mean(policy, axis=0)\n",
    "            policy = (\n",
    "                2 * (policy - np.min(policy)) / (np.max(policy) - np.min(policy)) - 1\n",
    "            )\n",
    "            policy = policy.reshape(NX, NV)\n",
    "            policies[f\"i={iteration}\"] = policy\n",
    "        plot_on_grid(\n",
    "            policies,\n",
    "            shared_cmap=len(BELLMAN_ITERATIONS_FOR_POLICY) > 1,\n",
    "            cmap=\"PRGn\",\n",
    "            title=f\"Policy for experiment = {experiment}, $\\pi_i(s)$\",\n",
    "            fontsize=20,\n",
    "            title_fontsize=30,\n",
    "        ).savefig(\n",
    "            os.path.join(BASE_PATH, experiment, f\"policy.pdf\"), bbox_inches=\"tight\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $V^{*} - V^{\\pi_i}$ on grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS) > 0:\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        perf_losses = {}\n",
    "        for iteration in BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS:\n",
    "            n_seeds = len(metrics[experiment][\"q_grid\"])\n",
    "            v_perf = np.zeros((n_seeds, NX * NV))\n",
    "            for idx_seed, seed_run in enumerate(experiment_seed_folder_map[experiment]):\n",
    "                v_pi_i = metrics[experiment][\"q_pi\"][seed_run][\n",
    "                    np.arange(n_bellman_iterations)[:, None],\n",
    "                    np.arange(NX * NV),\n",
    "                    np.argmax(\n",
    "                        metrics[experiment][\"q_grid\"][seed_run][iteration], axis=-1\n",
    "                    ),\n",
    "                ][iteration]\n",
    "                v_perf[idx_seed] = OPT_V.reshape(-1) - v_pi_i\n",
    "            v_perf = np.mean(v_perf, axis=0)\n",
    "            v_perf = v_perf.reshape(NX, NV)\n",
    "            v_perf = v_perf * (metrics[experiment][\"scaling\"] > 0).astype(float)\n",
    "            perf_losses[f\"i={iteration}\"] = v_perf\n",
    "\n",
    "        plot_on_grid(\n",
    "            perf_losses,\n",
    "            shared_cmap=len(BELLMAN_ITERATIONS_FOR_PERFORMANCE_LOSS) > 1,\n",
    "            title=f\"Performance loss for experiment = {experiment}\"\n",
    "            + \", $V^{*} - V^{\\pi_i}$\",\n",
    "            fontsize=20,\n",
    "            title_fontsize=30,\n",
    "        ).savefig(\n",
    "            os.path.join(BASE_PATH, experiment, f\"performance_loss_v.pdf\"),\n",
    "            bbox_inches=\"tight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $\\Vert Q_{i} - Q^{\\pi_i}\\Vert_{2, \\mu_D}$ on grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERESTIMATION_ERROR:\n",
    "    est_err = {}\n",
    "    for experiment in experiment_seed_folder_map:\n",
    "        n_seeds = len(metrics[experiment][\"q_grid\"])\n",
    "        n_bellman_iterations = list(metrics[experiment][\"q_grid\"].values())[0].shape[0]\n",
    "        est_err[experiment] = np.zeros((n_seeds, n_bellman_iterations))\n",
    "        for idx_seed, seed_run in enumerate(experiment_seed_folder_map[experiment]):\n",
    "            est_err[experiment][idx_seed] = np.sqrt(\n",
    "                np.sum(\n",
    "                    np.square(\n",
    "                        metrics[experiment][\"q_grid\"][seed_run]\n",
    "                        - metrics[experiment][\"q_pi\"][seed_run]\n",
    "                    )\n",
    "                    * metrics[experiment][\"scaling\"].reshape(-1)[:, np.newaxis],\n",
    "                    axis=(1, 2),\n",
    "                )\n",
    "            )\n",
    "    plot_value(\n",
    "        \"Bellman iteration\",\n",
    "        \" $\\Vert Q_{i} - Q^{\\pi_i}\\Vert_{2, \\mu_D}$\",\n",
    "        np.arange(0, n_bellman_iterations).tolist(),\n",
    "        est_err,\n",
    "        ticksize=10,\n",
    "        title=\"Overestimation error\",\n",
    "        fontsize=20,\n",
    "        linewidth=3,\n",
    "        xlim=(0, n_bellman_iterations),\n",
    "        xticks=[0]\n",
    "        + [\n",
    "            idx * 10 ** (int(np.log10(n_bellman_iterations)))\n",
    "            for idx in range(\n",
    "                1,\n",
    "                (n_bellman_iterations // (10 ** (int(np.log10(n_bellman_iterations)))))\n",
    "                + 1,\n",
    "            )\n",
    "        ],\n",
    "    ).savefig(\n",
    "        os.path.join(BASE_PATH, experiments_to_run[0], \"overestimation_error_q.pdf\"),\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
